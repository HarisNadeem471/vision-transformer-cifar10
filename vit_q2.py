# -*- coding: utf-8 -*-
"""vit-q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Cai9JQFm6w5OSF-kj5lHnU_6M5KX7SH
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Define data transformations (including normalization and augmentation)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalization (mean, std for CIFAR-10)
])

# Download CIFAR-10 dataset
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Create DataLoader for training and testing
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)

import torch.nn as nn
import torch

class PatchEmbedding(nn.Module):
    def __init__(self, patch_size, embed_dim):
        super(PatchEmbedding, self).__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.conv = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.conv(x)  # Split image into patches and embed them
        x = x.flatten(2)  # Flatten the patches
        x = x.transpose(1, 2)  # Rearrange shape (batch, num_patches, embed_dim)
        return x

import math

class PositionalEncoding(nn.Module):
    def __init__(self, num_patches, embed_dim):
        super(PositionalEncoding, self).__init__()

        # Create a positional encoding matrix (learned or fixed)
        pe = torch.zeros(num_patches, embed_dim)
        position = torch.arange(0, num_patches, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # Add batch dimension
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe

class TransformerEncoder(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim):
        super(TransformerEncoder, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        self.layernorm1 = nn.LayerNorm(embed_dim)
        self.layernorm2 = nn.LayerNorm(embed_dim)
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.1)

    def forward(self, x):
        # Multihead Attention
        attn_output, _ = self.attn(x, x, x)
        attn_output = self.dropout1(attn_output)
        x = self.layernorm1(x + attn_output)  # Skip connection

        # Feedforward Network
        ffn_output = self.ffn(x)
        ffn_output = self.dropout2(ffn_output)
        x = self.layernorm2(x + ffn_output)  # Skip connection
        return x

class VisionTransformer(nn.Module):
    def __init__(self, num_patches, embed_dim, num_heads, ff_dim, num_classes, patch_size=16):
        super(VisionTransformer, self).__init__()
        self.patch_embed = PatchEmbedding(patch_size=patch_size, embed_dim=embed_dim)
        self.pos_enc = PositionalEncoding(num_patches=num_patches, embed_dim=embed_dim)
        self.encoder = TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)

        # No need for global average pooling if you're flattening after the transformer encoder
        self.fc = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        patches = self.patch_embed(x)
        patches = self.pos_enc(patches)
        encoded = self.encoder(patches)

        # Flatten the output of the transformer encoder directly
        x = encoded.mean(dim=1)  # Aggregate across patches, resulting in shape [batch_size, embed_dim]
        #print(f"Shape after flattening: {x.shape}")  # Should be [batch_size, embed_dim]

        return self.fc(x)  # Fully connected layer for classification

# Model hyperparameters
embed_dim = 128
num_heads = 16
ff_dim = 256
num_classes = 10  # CIFAR-10 has 10 classes
num_patches = (32 // 16) * (32 // 16)  # CIFAR-10 images are 32x32, patch size is 16

# Initialize the Vision Transformer model
model = VisionTransformer(num_patches=num_patches, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, num_classes=num_classes)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)

def train_model(model, train_loader, criterion, optimizer, epochs=15):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_loader)
        epoch_accuracy = 100 * correct / total
        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

    # Save the model after training
    torch.save(model.state_dict(), 'vit_model.pth')
    print("Model saved successfully!")

# Train the model
model.cuda()  # Move model to GPU if available
train_model(model, train_loader, criterion, optimizer, epochs=10)

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, test_loader):
    model.eval()  # Set the model to evaluation mode
    y_true = []
    y_pred = []

    with torch.no_grad():  # Disable gradient computation
        for images, labels in test_loader:
            images, labels = images.cuda(), labels.cuda()  # Move to GPU if available
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    return cm

def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

# CIFAR-10 class names
class_names = [
    'airplane', 'automobile', 'bird', 'cat',
    'deer', 'dog', 'frog', 'horse',
    'ship', 'truck'
]

# Evaluate the model and plot the confusion matrix
cm = evaluate_model(model, test_loader)
plot_confusion_matrix(cm, class_names)

import torch
import torch.nn as nn
import torch.nn.functional as F

class HybridCNNMLP(nn.Module):
    def __init__(self, num_classes=10):
        super(HybridCNNMLP, self).__init__()

        # Convolutional layers for feature extraction
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        # Fully connected layers for classification
        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Adjust based on the output size of the CNN
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, num_classes)

    def forward(self, x):
        # CNN feature extraction
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))

        # Flatten the features for the MLP
        x = x.view(x.size(0), -1)  # Flatten the tensor

        # MLP for classification
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

# Model initialization
model = HybridCNNMLP(num_classes=10)

# Move the model to GPU if available
model.cuda()

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the hybrid model
train_model(model, train_loader, criterion, optimizer, epochs=10)

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader

# Load the pretrained ResNet model
resnet = models.resnet18(pretrained=True)  # You can use resnet18, resnet34, resnet50, etc.

# Freeze all layers in the pretrained model
for param in resnet.parameters():
    param.requires_grad = False

# Modify the final layer to match the number of classes in CIFAR-10
num_classes = 10  # CIFAR-10 has 10 classes
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)

# Move the model to GPU if available
resnet = resnet.cuda()

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(resnet.fc.parameters(), lr=0.001)  # Only train the new classifier layer

# Data augmentation and normalization for CIFAR-10
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])
])

# Load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Training function
def train_resnet_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.cuda(), labels.cuda()

            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_loader)
        epoch_accuracy = 100 * correct / total
        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

# Train the ResNet model
train_resnet_model(resnet, train_loader, criterion, optimizer, epochs=10)

import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt

# Check if GPU is available and use it
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize your Vision Transformer model with the same parameters used during training
embed_dim = 128  # Same as used during training
num_heads = 16
ff_dim = 256
num_classes = 10
num_patches = (32 // 16) * (32 // 16)

# Assuming the VisionTransformer class is already defined
model = VisionTransformer(
    num_patches=num_patches,
    embed_dim=embed_dim,
    num_heads=num_heads,
    ff_dim=ff_dim,
    num_classes=num_classes
)

# Load the saved model state
model_path = 'vit_model.pth'  # Update with your actual model path in Kaggle
model.load_state_dict(torch.load(model_path, map_location=device))
model = model.to(device)  # Move the model to GPU
model.eval()  # Set the model to evaluation mode

# Define the class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Define image transformations
transform = transforms.Compose([
    transforms.Resize((32, 32)),  # Resize the image to match your model's input size
    transforms.ToTensor()
])

# Load and preprocess the image
image_path = '/kaggle/input/your-image-directory/cat.jfif'  # Update with your actual image path in Kaggle
image = Image.open(image_path)
input_tensor = transform(image).unsqueeze(0)  # Add batch dimension
input_tensor = input_tensor.to(device)  # Move the input tensor to GPU

# Run inference
with torch.no_grad():
    output = model(input_tensor)
    prediction = torch.argmax(output, 1).item()

# Print the prediction as class name
print(f'Predicted Class: {class_names[prediction]}')

# Display the image with the predicted class
plt.imshow(image)
plt.title(f'Predicted Class: {class_names[prediction]}')
plt.axis('off')
plt.show()




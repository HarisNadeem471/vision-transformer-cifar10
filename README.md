# ğŸ–¼ï¸ Vision Transformer (ViT) for CIFAR-10 Image Classification  

This project implements a **Vision Transformer (ViT)** for **image classification** on the **CIFAR-10 dataset**. 
The model is compared with other architectures, including **ResNet and CNN-MLP hybrids**, to evaluate the effectiveness of Transformers in computer vision tasks.  

---

## ğŸš€ **Features**  
âœ” Preprocesses the **CIFAR-10 dataset**  
âœ” Applies **data augmentation techniques** (random cropping, flipping, normalization)  
âœ” Implements **Vision Transformer (ViT) from scratch** using a deep learning framework  
âœ” Divides images into **patches** and applies **positional encoding**  
âœ” Trains the ViT model using **multi-head self-attention & feed-forward layers**  
âœ” Tunes **hyperparameters** (layers, attention heads, patch size, learning rate)  
âœ” Compares performance with **CNN-based hybrid models & pretrained ResNet (transfer learning)**  
âœ” Evaluates models using **accuracy, precision, recall, F1-score, and confusion matrix**  
âœ” Plots **training & validation accuracy/loss curves**  
âœ” Deploys the models for **classification of test images**  

---

## ğŸ”§ **Requirements**  
- Python  
- TensorFlow/PyTorch  
- NumPy & Pandas  
- Matplotlib & Seaborn  
- OpenCV  

---

## ğŸ“Š **Results & Evaluation**  
The models were trained on the **CIFAR-10 dataset** and evaluated using:  
âœ” **Classification Accuracy**  
âœ” **Precision & Recall**  
âœ” **F1-score**  
âœ” **Confusion Matrix for misclassifications**  

A comparison was made between:  
1. **Vision Transformer (ViT) from scratch**  
2. **Hybrid CNN-MLP model**  
3. **Pretrained ResNet with transfer learning**  
